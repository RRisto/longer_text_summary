{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "id": "0dJgz8VuMRG7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c6ba39ec-ede9-47db-cc7f-c12356e6fdf3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install optimum[exporters]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqFdYdK8iRpM",
    "outputId": "fa43c6cc-c183-4f85-c387-8555f40a745f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting optimum[exporters]\n",
      "  Downloading optimum-1.19.1-py3-none-any.whl (417 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m417.0/417.0 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting coloredlogs (from optimum[exporters])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[exporters]) (1.12)\n",
      "Requirement already satisfied: transformers[sentencepiece]<4.41.0,>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum[exporters]) (4.38.2)\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum[exporters]) (2.2.1+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum[exporters]) (24.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum[exporters]) (1.25.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum[exporters]) (0.20.3)\n",
      "Collecting datasets (from optimum[exporters])\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m542.0/542.0 kB\u001B[0m \u001B[31m28.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting onnx (from optimum[exporters])\n",
      "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.9/15.9 MB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting onnxruntime (from optimum[exporters])\n",
      "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.8/6.8 MB\u001B[0m \u001B[31m80.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting timm (from optimum[exporters])\n",
      "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m76.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[exporters]) (4.10.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[exporters]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[exporters]) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m14.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m15.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m24.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m696.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m8.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[exporters]) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum[exporters])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m34.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[exporters]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[exporters]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[exporters]) (0.4.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[exporters]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum[exporters]) (3.20.3)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum[exporters])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[exporters]) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[exporters]) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum[exporters])\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[exporters]) (1.5.3)\n",
      "Collecting xxhash (from datasets->optimum[exporters])\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m18.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting multiprocess (from datasets->optimum[exporters])\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m15.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[exporters]) (3.9.3)\n",
      "Collecting huggingface-hub>=0.8.0 (from optimum[exporters])\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m388.9/388.9 kB\u001B[0m \u001B[31m31.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->optimum[exporters]) (24.3.25)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[exporters]) (1.3.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->optimum[exporters]) (0.17.1+cu121)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[exporters]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[exporters]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[exporters]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[exporters]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[exporters]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[exporters]) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[exporters]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum[exporters]) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[exporters]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[exporters]) (2023.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm->optimum[exporters]) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[exporters]) (1.16.0)\n",
      "Installing collected packages: xxhash, onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, coloredlogs, onnxruntime, nvidia-cusolver-cu12, datasets, timm, optimum\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed coloredlogs-15.0.1 datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 humanfriendly-10.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 onnx-1.16.0 onnxruntime-1.17.3 optimum-1.19.1 timm-0.9.16 xxhash-3.4.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgdIdpqJLcQX",
    "outputId": "6dcf74da-2f56-46be-ad76-dc6c28094096",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/84.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━\u001B[0m \u001B[32m81.9/84.1 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.22.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install rouge_score"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41mjtJeDLwGl",
    "outputId": "085e78e2-e79f-420f-b10b-6282cdac2a78",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=3faed24ad947c1d434166acfb5655563f4998a372b64cc402ad204aa1ed7343c\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "id": "OEn_rjaTNwU3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## convert longtt5 to onnx runtime"
   ],
   "metadata": {
    "id": "ztoPyuGYKgGr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! optimum-cli export onnx --model drive/MyDrive/LLM/mlong-t5-tglobal-et-base --task text2text-generation drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siFZav2DE0S8",
    "outputId": "5d7b66ea-e15b-4bba-b47c-c7e229c58991",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2024-04-25 16:31:34.399426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-25 16:31:34.399505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-25 16:31:34.402044: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-25 16:31:36.194818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Framework not specified. Using pt to export the model.\n",
      "The task `text2text-generation` was manually specified, and past key values will not be reused in the decoding. if needed, please pass `--task text2text-generation-with-past` to export using the past key values.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 2048}\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "\n",
      "***** Exporting submodel 1/2: LongT5Stack *****\n",
      "Using framework PyTorch: 2.2.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:175: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  _global_block_ids_lower_bound = torch.tensor(-1, dtype=global_block_ids.dtype, device=global_block_ids.device)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:185: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_globals > 0:\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:212: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  block_ids >= 0, torch.tensor(global_seq_len, dtype=block_ids.dtype, device=block_ids.device)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:212: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  block_ids >= 0, torch.tensor(global_seq_len, dtype=block_ids.dtype, device=block_ids.device)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:79: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape[dim] % block_len != 0:\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:62: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not all(x.shape):\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:84: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if 0 in output_shape:\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:2174: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "\n",
      "***** Exporting submodel 2/2: LongT5ForConditionalGeneration *****\n",
      "Using framework PyTorch: 2.2.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1005: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1017: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n",
      "Post-processing the exported models...\n",
      "Deduplicating shared (tied) weights...\n",
      "Could not find ONNX initializer for torch parameter encoder.embed_tokens.weight. encoder.embed_tokens.weight will not be checked for deduplication.\n",
      "Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n",
      "\tdecoder.embed_tokens.weight: {'shared.weight'}\n",
      "\tencoder.embed_tokens.weight: set() --> ignored (may be a parameter from a part of the model not exported)\n",
      "\tshared.weight: {'shared.weight'}\n",
      "\n",
      "Validating ONNX model drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx/encoder_model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (last_hidden_state)\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (2, 16, 768) matches (2, 16, 768)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\n",
      "Validating ONNX model drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx/decoder_model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (logits)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 16, 18171) matches (2, 16, 18171)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "The ONNX export succeeded and the exported model was saved at: drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ls -lah drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQQF_CF5E6k0",
    "outputId": "a3084289-38af-42b3-d9c4-9cbc115d7e50",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 920M\n",
      "-rw------- 1 root root  910 Apr 25 16:32 config.json\n",
      "-rw------- 1 root root 539M Apr 25 16:32 decoder_model.onnx\n",
      "-rw------- 1 root root 380M Apr 25 16:32 encoder_model.onnx\n",
      "-rw------- 1 root root  134 Apr 25 16:32 generation_config.json\n",
      "-rw------- 1 root root  416 Apr 25 16:32 special_tokens_map.json\n",
      "-rw------- 1 root root 302K Apr 25 16:32 spiece.model\n",
      "-rw------- 1 root root  861 Apr 25 16:32 tokenizer_config.json\n",
      "-rw------- 1 root root 1.1M Apr 25 16:32 tokenizer.json\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load onnx model"
   ],
   "metadata": {
    "id": "ifXWfdj_KbNs",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(model_id, use_cache=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_gKKzw9Hbs7",
    "outputId": "505eda1d-5d04-4e00-c12f-95c320a43ba5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## summarize text"
   ],
   "metadata": {
    "id": "LDlJGhI-KYQl",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def summarize(text, model, tokenizer, max_new_tokens=512, device='cpu'):\n",
    "    input_ids = tokenizer(\n",
    "         text, return_tensors=\"pt\"\n",
    "    ).input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids=input_ids.to(device), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "id": "GCEjPGe-H311",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text=\"\"\"Veel veebruaris oli Soome kõige populaarsem partei Koonderakond, kuid kaotas märtsis selle tiitli SDP-le. Märtsis langes peaministripartei  toetus ühe protsendi võrra ning Koonderakonna toetus on nüüd 20,6 protsenti.\n",
    "SDP suurendas toetust naiste ja noorte hulgas. Märtsis tõusis SDP toetus 1,9 protsenti ning erakonna toetus on nüüd 21,7 protsenti.\n",
    "Koonderakonna kannul on Põlissoomlased, rahandusminister Riikka Purra kodupartei toetus on 17,4 protsenti.\"\"\"\n",
    "summarize(text, model, tokenizer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wXIqcm0I5dX",
    "outputId": "979955c8-004f-4404-c2bb-04c99047f575",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "- Soome Koonderakond kaotas märtsis 10,6 protsenti SDP-le<0x0A>- Koonderakonna toetus langes peaministripartei toetusele<0x0A>- Erakonna toetus tõusis 1,9 protsenti ning Koonderakonna kannul on Põlissoomlased\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df=pd.read_excel('drive/MyDrive/LLM/data/riigikogu/for_summarization_mbart_2048_chunks_summaries.xlsx')\n",
    "df_train=df[df.split=='train']\n",
    "df_test=df[df.split=='test']"
   ],
   "metadata": {
    "id": "Ay3ZJMj1JpL-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### evaluation"
   ],
   "metadata": {
    "id": "DkJCKC3uLFLt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ],
   "metadata": {
    "id": "F1Fj11WOLN7R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calc_texts_labels_metrics(texts, summaries, max_input_length=2048, max_output_length=512, batch_size = 10, device='cpu'):\n",
    "    true_labels=tokenizer(\n",
    "        summaries, return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids.cpu()\n",
    "    input_ids = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(0, input_ids.size(0), batch_size):\n",
    "        batch_input_ids = input_ids[i:i + batch_size].to(device)\n",
    "        batch_outputs = model.generate(input_ids=batch_input_ids, max_length=max_output_length)\n",
    "        predictions.extend(batch_outputs.cpu().detach().numpy())\n",
    "    max_length = max(len(p) for p in predictions)\n",
    "    padded_predictions = [np.pad(p, (0, max_length - len(p)), mode='constant') for p in predictions]\n",
    "    outputs = torch.tensor(padded_predictions)\n",
    "    # outputs = torch.tensor(predictions)\n",
    "    eval_preds = (outputs, true_labels.cpu())\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    return metrics"
   ],
   "metadata": {
    "id": "P0E-IMGaLplP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%time eval_metrics=calc_texts_labels_metrics(df_test.text.tolist(), df_test.summary.tolist())"
   ],
   "metadata": {
    "id": "xwqrnu7hMC3x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "jzFjAYx_MWTr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantize"
   ],
   "metadata": {
    "id": "q1bK3VI9Fg0C",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqOJbMHTFoqW",
    "outputId": "b10a8b2c-9dc2-4e7a-e009-bf0e6c2e04e7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!optimum-cli onnxruntime quantize \\\n",
    "  --avx512 \\\n",
    "  --onnx_model drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx \\\n",
    "  -o drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx-quantized"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQ32zyWFFiOw",
    "outputId": "6f85e7a5-a3c0-43f3-d37c-573305052511",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2024-04-26 06:33:40.252313: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-26 06:33:40.252388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-26 06:33:40.254048: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-26 06:33:41.792961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Tile_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__44\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__58\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__72\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_4_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_8_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_12_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__129\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_13_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 3\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Where_5_output_0', type is tensor_type {\n",
      "  elem_type: 11\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__149\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 1\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__150\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 128\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 384\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Slice_17_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__168\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.0/layer.0/TransientGlobalSelfAttention/Reshape_25_output_0', type is tensor_type {\n",
      "  elem_type: 11\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__174\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__175\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__176\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__177\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__178\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.1/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__214\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.1/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__228\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.1/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__242\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.1/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.1/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.2/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__330\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.2/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__344\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.2/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__358\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.2/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.2/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.3/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__446\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.3/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__460\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.3/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__474\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.3/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.3/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.4/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__562\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.4/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__576\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.4/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__590\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.4/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.4/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.5/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__678\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.5/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__692\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.5/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__706\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.5/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.5/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.6/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__794\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.6/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__808\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.6/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__822\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.6/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.6/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.7/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__910\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.7/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__924\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.7/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__938\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.7/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.7/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.8/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1026\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.8/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1040\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.8/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1054\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.8/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.8/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.9/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1142\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.9/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1156\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.9/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1170\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.9/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.9/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.10/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1258\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.10/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1272\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.10/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1286\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.10/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.10/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.11/layer.0/TransientGlobalSelfAttention/Slice_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1374\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.11/layer.0/TransientGlobalSelfAttention/Slice_1_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1388\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.11/layer.0/TransientGlobalSelfAttention/Slice_2_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"unk__1402\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.11/layer.0/TransientGlobalSelfAttention/Slice_3_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/block.11/layer.0/TransientGlobalSelfAttention/Slice_7_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 5\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n",
      "Saving quantized model at: drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx-quantized (external data format: False)\n",
      "Configuration saved in drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx-quantized/ort_config.json\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 2048}\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx-quantized (external data format: False)\n",
      "Configuration saved in drive/MyDrive/LLM/mlong-t5-tglobal-et-base-onnx-quantized/ort_config.json\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 2048}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8h-HcPd_F30H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}