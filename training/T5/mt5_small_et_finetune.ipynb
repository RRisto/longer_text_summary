{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30935c25-5c96-476f-a0b0-b52de2cab591",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/risto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import GenerationConfig\n",
    "from random import randrange\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3ccec9-1a90-4e52-93df-277c06e0dedd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e69b14-4a03-463e-b71b-79c6b800edc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf42950-9405-4d2a-bb56-46991f67c626",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c325f1-27e3-46af-9918-72289c66fad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad355d6-5bea-4ecf-9a4e-41e1355fe940",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f56bd08-ae69-449c-8f1f-8705921014fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a35e3-6189-4778-a87b-9f5ec059b1a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "source: https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6685f2-29ac-4018-9486-b33f823f014a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df83dd6-353c-4e01-874b-1fb76d377560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"google/mt5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd92d3c-0e82-4063-b989-e0baa11c4fa6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4266064454395085\n",
      "0.4266064454395085\n"
     ]
    }
   ],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "print(msize(model.shared) / msize(model))   \n",
    "print(msize(model.lm_head) / msize(model))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16672a47-8e1b-4318-bf3f-6ec962315d35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "about 42% are embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2ca13-1507-402d-bb77-289d594c6219",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0488a3e2-a550-4014-9f58-25686dd78a8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"TalTechNLP/samsum_ee\"\n",
    "\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0af9bcd-5210-42f7-ba0a-08d30b42886a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 13199\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2ad20f-2136-4d1e-af02-960588efe7a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 809\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset['test'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff097111-2531-4ce9-ab97-510ef699fa4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## update model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3658a305-6512-4b78-a6ee-64fdccf3780d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26398"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts4vocab=train_dataset['dialogue']+train_dataset['summary']\n",
    "len(texts4vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19cfcdd-c6da-4ab7-812a-5bd2c09efa86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22815 0.0912235105957617\n"
     ]
    }
   ],
   "source": [
    "cnt_et = Counter()\n",
    "for text in texts4vocab:\n",
    "    cnt_et.update(tokenizer.encode(text))\n",
    "print(len(cnt_et), len(cnt_et)/tokenizer.vocab_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590004ae-8bd8-471b-a3df-f4e4ced08f35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(259, 222378),\n",
       " (267, 152951),\n",
       " (260, 109486),\n",
       " (261, 103717),\n",
       " (351, 37623),\n",
       " (291, 35910),\n",
       " (262, 29563),\n",
       " (1, 26398),\n",
       " (263, 24319),\n",
       " (432, 24040),\n",
       " (265, 23799),\n",
       " (309, 21963),\n",
       " (266, 21379),\n",
       " (383, 20708),\n",
       " (327, 19689),\n",
       " (316, 18386),\n",
       " (1055, 17737),\n",
       " (496, 17714),\n",
       " (1221, 15946),\n",
       " (178005, 14920)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_et.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b33882-e523-47fb-82d6-882dde21115b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23422\n"
     ]
    }
   ],
   "source": [
    "new_tokens = set(range(1000))\n",
    "for i, (k, v) in enumerate(cnt_et.items()):\n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "for t in range(tokenizer.vocab_size - 100, tokenizer.vocab_size):\n",
    "    new_tokens.add(t)\n",
    "print(len(new_tokens))\n",
    "kept_ids = sorted(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed5e901-6a7d-44d4-a9e7-b3616499d6fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[250090,\n",
       " 250091,\n",
       " 250092,\n",
       " 250093,\n",
       " 250094,\n",
       " 250095,\n",
       " 250096,\n",
       " 250097,\n",
       " 250098,\n",
       " 250099]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_ids[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808335c-9428-4102-bb58-db3e7d214081",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### update model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4c275e-2b32-46ee-ac7a-d06a6ef72f3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_size = len(kept_ids)\n",
    "new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)\n",
    "new_head = torch.nn.Linear(in_features=model.lm_head.in_features, out_features=new_size, bias=False)\n",
    "for new_id, old_id in enumerate(kept_ids):\n",
    "    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]\n",
    "    new_head.weight.data[new_id] = model.lm_head.weight.data[old_id]\n",
    "model.shared.weight = new_emb.weight\n",
    "model.lm_head.weight = new_head.weight\n",
    "model.config.__dict__['vocab_size'] = new_size\n",
    "model.config.__dict__['_name_or_path'] = 'mt5_et/mt_et_t5-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f2fad-46bb-49d6-a79e-12713f02d131",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### update tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "400f729c-00f5-47aa-a2b2-ae51c67a1866",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"
     ]
    }
   ],
   "source": [
    "!apt-get update \n",
    "!apt install protobuf-compiler -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03c511e8-c156-4f7a-8bcb-0719fa8610d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-10 06:47:01--  https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14023 (14K) [text/plain]\n",
      "Saving to: ‚Äòsentencepiece_model.proto.3‚Äô\n",
      "\n",
      "sentencepiece_model 100%[===================>]  13.69K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-04-10 06:47:01 (20.3 MB/s) - ‚Äòsentencepiece_model.proto.3‚Äô saved [14023/14023]\n",
      "\n",
      "the loaded model has pieces: 250100\n",
      "the new pieces: 23422\n",
      "23422\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto\n",
    "! protoc --python_out=. sentencepiece_model.proto\n",
    "import sentencepiece.sentencepiece_model_pb2 as spmp\n",
    "smp = tokenizer.sp_model.serialized_model_proto()\n",
    "m = spmp.ModelProto()\n",
    "m.ParseFromString(smp)\n",
    "print('the loaded model has pieces:', len(m.pieces))\n",
    "new_pieces = [m.pieces[idx] for idx in kept_ids]\n",
    "print('the new pieces:', len(new_pieces))\n",
    "# replace the content of the first 30K pieces\n",
    "for i, p in enumerate(new_pieces):\n",
    "    m.pieces[i].piece = p.piece\n",
    "    m.pieces[i].score = p.score\n",
    "    m.pieces[i].type = p.type\n",
    "# drop the remaining pieces\n",
    "n = len(new_pieces)\n",
    "for i in range(len(m.pieces) - n):\n",
    "    m.pieces.pop(len(m.pieces) - 1)\n",
    "print(len(m.pieces))\n",
    "with open('mt5_et/new_sp.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())\n",
    "new_tokenizer = T5Tokenizer('mt5_et/new_sp.model', extra_ids=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0af3370-804f-412b-ab64-d1f671357f54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained('mt5_et')\n",
    "model.save_pretrained('mt5_et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ca62b8f-37bf-4876-9a31-a3c7752f7701",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('mt5_et')\n",
    "model = T5ForConditionalGeneration.from_pretrained('mt5_et', max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c1b3b-d201-49bf-9362-85cfc93c3366",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## prep data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58462ab6-a200-4870-9404-116e6e9dd483",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_source_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f000e83a-938f-4062-bbd9-81e49581f3bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_target_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3737edea-df3e-4261-9141-ad2c8368fbd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23785d0a676343b48ae186557665d5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780e8218947c479fa2ad70a12cafbeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/809 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc810e-4647-46f0-8813-a0ac8f5f5a9e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de471419-c631-41db-9055-485596a606d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bcf0960-da63-4f4f-a88b-6bd8cdce783d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c1268-ae7d-4537-9de0-f6b448a5af76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9394d21f-92d3-42fa-8597-a2ff3c8aad64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStoppingCallback(3, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c4ae189-8f5e-4add-8b34-4cf0c40c2ce6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"#for some reason this was not working\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=30,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    "    # generation_max_length=40\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "027c86c4-b33e-429c-8a8a-e9d975162bcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49500' max='49500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49500/49500 6:51:57, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.613500</td>\n",
       "      <td>2.712732</td>\n",
       "      <td>25.906500</td>\n",
       "      <td>8.160100</td>\n",
       "      <td>22.662800</td>\n",
       "      <td>24.040100</td>\n",
       "      <td>18.700865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.224900</td>\n",
       "      <td>2.552037</td>\n",
       "      <td>28.076700</td>\n",
       "      <td>9.282900</td>\n",
       "      <td>24.455800</td>\n",
       "      <td>26.048200</td>\n",
       "      <td>23.902349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.010400</td>\n",
       "      <td>2.460055</td>\n",
       "      <td>29.156700</td>\n",
       "      <td>10.205600</td>\n",
       "      <td>25.723000</td>\n",
       "      <td>27.350600</td>\n",
       "      <td>29.870210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.873300</td>\n",
       "      <td>2.387637</td>\n",
       "      <td>30.111000</td>\n",
       "      <td>10.863300</td>\n",
       "      <td>26.417800</td>\n",
       "      <td>28.336500</td>\n",
       "      <td>29.226205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.768400</td>\n",
       "      <td>2.346879</td>\n",
       "      <td>29.914600</td>\n",
       "      <td>11.152300</td>\n",
       "      <td>26.280800</td>\n",
       "      <td>28.139400</td>\n",
       "      <td>25.719407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.698200</td>\n",
       "      <td>2.318779</td>\n",
       "      <td>31.321600</td>\n",
       "      <td>11.811700</td>\n",
       "      <td>27.484300</td>\n",
       "      <td>29.489300</td>\n",
       "      <td>25.085290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.615400</td>\n",
       "      <td>2.297585</td>\n",
       "      <td>30.961700</td>\n",
       "      <td>11.680500</td>\n",
       "      <td>27.162100</td>\n",
       "      <td>29.244900</td>\n",
       "      <td>28.719407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.555300</td>\n",
       "      <td>2.264596</td>\n",
       "      <td>31.558100</td>\n",
       "      <td>12.171000</td>\n",
       "      <td>27.651200</td>\n",
       "      <td>29.648100</td>\n",
       "      <td>25.922126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.528400</td>\n",
       "      <td>2.245093</td>\n",
       "      <td>32.059300</td>\n",
       "      <td>12.613200</td>\n",
       "      <td>27.815100</td>\n",
       "      <td>30.009100</td>\n",
       "      <td>28.139679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.477300</td>\n",
       "      <td>2.239874</td>\n",
       "      <td>31.788800</td>\n",
       "      <td>12.324000</td>\n",
       "      <td>27.475800</td>\n",
       "      <td>29.657500</td>\n",
       "      <td>26.331273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.430200</td>\n",
       "      <td>2.214731</td>\n",
       "      <td>32.095500</td>\n",
       "      <td>12.488900</td>\n",
       "      <td>27.741100</td>\n",
       "      <td>30.119300</td>\n",
       "      <td>27.473424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.412100</td>\n",
       "      <td>2.206002</td>\n",
       "      <td>32.120500</td>\n",
       "      <td>12.448300</td>\n",
       "      <td>27.742900</td>\n",
       "      <td>30.178400</td>\n",
       "      <td>29.410383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.356300</td>\n",
       "      <td>2.190853</td>\n",
       "      <td>32.792400</td>\n",
       "      <td>12.645900</td>\n",
       "      <td>28.218100</td>\n",
       "      <td>30.645700</td>\n",
       "      <td>26.954265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.341000</td>\n",
       "      <td>2.188806</td>\n",
       "      <td>33.112300</td>\n",
       "      <td>12.803300</td>\n",
       "      <td>28.387900</td>\n",
       "      <td>30.941400</td>\n",
       "      <td>29.767614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.330600</td>\n",
       "      <td>2.183685</td>\n",
       "      <td>32.745500</td>\n",
       "      <td>12.908800</td>\n",
       "      <td>28.203100</td>\n",
       "      <td>30.661300</td>\n",
       "      <td>29.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.295800</td>\n",
       "      <td>2.168080</td>\n",
       "      <td>33.042600</td>\n",
       "      <td>12.923700</td>\n",
       "      <td>28.384600</td>\n",
       "      <td>30.890700</td>\n",
       "      <td>29.665019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.275000</td>\n",
       "      <td>2.160700</td>\n",
       "      <td>33.082700</td>\n",
       "      <td>12.694300</td>\n",
       "      <td>28.517000</td>\n",
       "      <td>30.910500</td>\n",
       "      <td>29.825711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.253600</td>\n",
       "      <td>2.169038</td>\n",
       "      <td>33.057700</td>\n",
       "      <td>13.040100</td>\n",
       "      <td>28.656900</td>\n",
       "      <td>30.970400</td>\n",
       "      <td>28.840544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.232100</td>\n",
       "      <td>2.159625</td>\n",
       "      <td>33.239100</td>\n",
       "      <td>13.124900</td>\n",
       "      <td>28.665500</td>\n",
       "      <td>31.138200</td>\n",
       "      <td>29.196539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.212400</td>\n",
       "      <td>2.159690</td>\n",
       "      <td>33.066000</td>\n",
       "      <td>13.017100</td>\n",
       "      <td>28.614000</td>\n",
       "      <td>30.920600</td>\n",
       "      <td>30.682324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.189800</td>\n",
       "      <td>2.149160</td>\n",
       "      <td>33.369900</td>\n",
       "      <td>13.137100</td>\n",
       "      <td>28.925600</td>\n",
       "      <td>31.347100</td>\n",
       "      <td>29.855377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.193800</td>\n",
       "      <td>2.147640</td>\n",
       "      <td>32.944300</td>\n",
       "      <td>12.894500</td>\n",
       "      <td>28.500400</td>\n",
       "      <td>30.953700</td>\n",
       "      <td>30.948084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.163600</td>\n",
       "      <td>2.148372</td>\n",
       "      <td>33.446200</td>\n",
       "      <td>13.208200</td>\n",
       "      <td>28.777600</td>\n",
       "      <td>31.356300</td>\n",
       "      <td>31.180470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.166200</td>\n",
       "      <td>2.146236</td>\n",
       "      <td>33.570400</td>\n",
       "      <td>13.102200</td>\n",
       "      <td>28.879500</td>\n",
       "      <td>31.495600</td>\n",
       "      <td>29.144623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.164500</td>\n",
       "      <td>2.134749</td>\n",
       "      <td>33.779800</td>\n",
       "      <td>13.402400</td>\n",
       "      <td>29.134700</td>\n",
       "      <td>31.684400</td>\n",
       "      <td>28.945612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.158500</td>\n",
       "      <td>2.137822</td>\n",
       "      <td>33.546200</td>\n",
       "      <td>13.151700</td>\n",
       "      <td>28.999100</td>\n",
       "      <td>31.455000</td>\n",
       "      <td>29.046972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.159100</td>\n",
       "      <td>2.137201</td>\n",
       "      <td>33.497800</td>\n",
       "      <td>13.180300</td>\n",
       "      <td>28.840200</td>\n",
       "      <td>31.375100</td>\n",
       "      <td>29.630408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.142700</td>\n",
       "      <td>2.134053</td>\n",
       "      <td>33.628500</td>\n",
       "      <td>13.182600</td>\n",
       "      <td>28.892500</td>\n",
       "      <td>31.443900</td>\n",
       "      <td>29.482077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.142100</td>\n",
       "      <td>2.135680</td>\n",
       "      <td>33.472700</td>\n",
       "      <td>13.177600</td>\n",
       "      <td>28.794100</td>\n",
       "      <td>31.378900</td>\n",
       "      <td>30.009889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.149900</td>\n",
       "      <td>2.135469</td>\n",
       "      <td>33.771200</td>\n",
       "      <td>13.379200</td>\n",
       "      <td>29.035700</td>\n",
       "      <td>31.579500</td>\n",
       "      <td>29.080346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49500, training_loss=2.4860098420730745, metrics={'train_runtime': 24718.4609, 'train_samples_per_second': 16.019, 'train_steps_per_second': 2.003, 'total_flos': 1.363698115411968e+17, 'train_loss': 2.4860098420730745, 'epoch': 30.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa402a3-35c1-48bb-9e33-c09ba525d4c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825c6dbc-ae7d-426d-96e4-9e8d14376e8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_id='mt5-small-TalTechNLP/samsum_ee/checkpoint-46200/'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfeea40b-7be7-42d5-8d77-3f92872cd9f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model=model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07061c04-ac02-42fe-8bd9-c0a588dcf9bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_texts_labels_metrics(texts, summaries, model, tokenizer, max_input_length=1024, max_output_length=512, batch_size = 10, temperature=0):\n",
    "    true_labels=tokenizer(\n",
    "        summaries, return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids.cpu()\n",
    "    input_ids = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids  \n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(0, input_ids.size(0), batch_size):\n",
    "        batch_input_ids = input_ids[i:i + batch_size].to('cuda')\n",
    "        batch_outputs = model.generate(input_ids=batch_input_ids, max_length=max_output_length, temperature=temperature)\n",
    "        predictions.extend(batch_outputs.cpu().detach().numpy())\n",
    "    max_length = max(len(p) for p in predictions)\n",
    "    padded_predictions = [np.pad(p, (0, max_length - len(p)), mode='constant') for p in predictions]\n",
    "    outputs = torch.tensor(padded_predictions)\n",
    "    eval_preds = (outputs, true_labels.cpu())\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1512cb-4013-4755-ae87-70a0e4d6f1ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_40602/2151791639.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  outputs = torch.tensor(padded_predictions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 32.7898,\n",
       " 'rouge2': 13.0878,\n",
       " 'rougeL': 28.3526,\n",
       " 'rougeLsum': 30.7215,\n",
       " 'gen_len': 31.597033374536466}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics=calc_texts_labels_metrics(test_dataset['dialogue'], test_dataset['summary'], model, tokenizer)\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c751f-8c86-4371-b961-994df10e6767",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a53e1a-8ee0-4450-8f90-794a997e9e27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def summarize(text, model, tokenizer, max_input_length=1024, max_new_tokens=512):\n",
    "    input_ids = tokenizer(\n",
    "         text, return_tensors=\"pt\",\n",
    "        max_length=max_input_length\n",
    "    ).input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids=input_ids.to('cuda'), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f305dd4a-1c05-490e-a936-3969ea516119",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda saadab Betty telefoninumbri, et saada talle s√µnumi. Betty helistas talle viimati, kui nad koos pargis olisid.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][0]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a603bd09-3b4e-47e8-bf78-2a8f6ccaf6ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah: Hei, kas sul on Betty number?\\nAmanda: Vaatan j√§rele.\\nHannah: <file_gif>\\nAmanda: Vabandust, ei leia seda.\\nAmanda: K√ºsi Larrylt.\\nAmanda: Ta helistas talle viimati, kui me koos pargis olime.\\nHannah: Ma ei tunne teda h√§sti.\\nHannah: <file_gif>\\nAmanda: √Ñra ole h√§belik, ta on v√§ga tore.\\nHannah: Kui sa √ºtled nii..\\nHannah: Ma eelistaksin, et sa talle s√µnumi saadaksid.\\nAmanda: Lihtsalt saada talle s√µnum üôÇ\\nHannah: Urgh.. Olgu siis\\nHannah: N√§gemist\\nAmanda: N√§gemist-n√§gemist'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beee675b-a0f3-4943-9951-ee948415a24e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah vajab Betty telefoninumbrit, kuid Amandal seda pole. Ta peab Larryga √ºhendust v√µtma.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2065e610-7af4-4e53-89a6-87bd119c1884",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eric ja Rob vaatavad rongi osa. Rob vaatab rongi osa Youtube'is.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][1]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3ff60ed-775b-40af-85fa-dcd1dfeea31e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric: MASIN!\\nRob: See on nii lahe!\\nEric: Ma tean! Ja n√§itab, kuidas ameeriklased n√§evad venelasi ;)\\nRob: Ja see on t√µesti naljakas!\\nEric: Ma tean! Mulle meeldib eriti rongi osa!\\nRob: Hahaha! Keegi ei r√§√§gi masinaga nii!\\nEric: Kas see on tema ainus etendus?\\nRob: Ei tea. Ma vaatan j√§rele.\\nEric: Kindlasti.\\nRob: Selgub, et ei! M√µned tema etendused on Youtube'is.\\nEric: Lahe! Ma vaatan neid kohe!\\nRob: Mina ka!\\nEric: MASIN!\\nRob: MASIN!\\nEric: Kuni kohtumiseni?\\nRob: Muidugi :)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][1]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb43273-7c09-4538-a2c0-bfa5a645eb49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric ja Rob l√§hevad vaatama stand-up'i YouTube'is.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][1]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b5c607-6e64-41b4-ae57-bd14dc9b5966",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenny soovib osta esimese v√µi kolmanda paari. Bob soovitab talle erinevaid riietumisv√µimalusi.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][2]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a95fc7-be0c-4cee-bb99-2a3ab7270c0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lenny: Kallis, kas sa saaksid mulle millegagi abi anda?\\nBob: Muidugi, mis toimub?\\nLenny: Millise ma peaksin valima?\\nBob: Saada mulle pilte.\\nLenny: <file_photo>\\nLenny: <file_photo>\\nLenny: <file_photo>\\nBob: Mulle meeldivad k√µige rohkem esimesed.\\nLenny: Aga mul on juba lillad p√ºksid. Kas on m√µtet omada kahte paari?\\nBob: Mul on neli musta paari :D :D\\nLenny: Jah, aga kas ma ei peaks valima erinevat v√§rvi?\\nBob: Oluline on see, milline annab sulle k√µige rohkem riietumisv√µimalusi.\\nLenny: Nii et ma arvan, et ostan esimese v√µi kolmanda paari siis.\\nBob: Vali parim kvaliteet siis.\\nLenny: Sul on √µigus, ait√§h.\\nBob: Pole probleemi :)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][2]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d79f947-cf55-4064-9eb8-504ea997bdd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lenny ei suuda otsustada, millised p√ºksid osta. Bob n√µustas Lennyt selles k√ºsimuses. Lenny l√§heb Bobi n√µu j√§rgi ja valib kvaliteetseimad p√ºksid.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][2]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a06f293e-aa4c-420f-a9d5-e12db54de64d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma on mures toiduvalmistamise p√§rast. Will j√µuab varsti koju, kuna tal pole isu.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][3]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcad0866-0c6d-47e1-9d58-489db0f887d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Will: hei kallis, mida sa t√§na √µhtuks s√º√ºa tahad?\\nEmma: oh, √§ra t√§na √ºldse muretse selle p√§rast\\nWill: mida sa selle all m√µtled? kas k√µik on korras?\\nEmma: mitte p√§ris, aga see on ok, √§ra muretse toiduvalmistamise p√§rast, mul pole isu\\nWill: millal sa koju j√µuad?\\nEmma: varsti, loodetavasti\\nWill: kas sa oled kindel? v√µib-olla tahad, et ma sind j√§rele tuleksin?\\nEmma: ei ei, see on korras. Ma j√µuan varsti koju, ma √ºtlen sulle, kui ma koju j√µuan.\\nWill: Selge, armastan sind.\\nEmma: armastan sind ka.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][3]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a5768b-4b69-4bad-9e2f-69def9a77906",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma tuleb varsti koju ja ta annab Willile teada.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][3]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d04131b-6ea2-4d9c-ae16-8e1d2789166d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane ja Ollie l√§hevad reedel Varssavis l√µunaks. Nad kohtuvad reedel kell 18.00 Marokosse.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][4]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2563d342-cb99-4ab3-9a71-b0ed2dd695f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ollie: Tere, kas sa oled Varssavis?\\nJane: Jah, just tagasi! Muide, kas sa oled vaba √µhtus√∂√∂giks 19. kuup√§eval?\\nOllie: Ei ole!\\nJane: Aga 18.?\\nOllie: Ei, meil on see pidu ja sa pead seal olema, m√§letad?\\nJane: Oh √µige! Ma kaotasin oma kalendri.. ait√§h, et meelde tuletasid.\\nOllie: Kas me l√µunatame sel n√§dalal?\\nJane: Hea meelega!\\nOllie: Reede?\\nJane: Ok.\\nJane: Mida sa m√µtled \"meil pole enam viskit!\" lol..\\nOllie: Mida!!!?\\nJane: Sa helistasid mulle ja ainus asi, mida ma kuulsin, oli see lause viski kohta... mis sinuga lahti on?\\nOllie: Oh oh... v√§ga imelik! Pean olema ettevaatlik, v√µib-olla on minu telefonis m√µni spioon! lol\\nJane: √Ñra muretse, me kontrollime reedel.\\nOllie: √Ñra unusta p√§ikest kaasa v√µtta.\\nJane: Ma ei j√µua √§ra oodata, et Marokosse j√µuda..\\nOllie: Naudi ja n√§eme reedel.\\nJane: Vabandust, Ollie, mul on v√§ga kiire, mul ei ole homme l√µunaks aega, aga ehk kell 18 p√§rast minu kursusi? See Maroko reis oli nii tore, aga aegan√µudev!\\nOllie: Ok, teeks siis teed!\\nJane: Olen teel..\\nOllie: Tee on valmis, kas t√µid k√ºpsetised kaasa?\\nJane: Ma s√µin need juba √§ra... n√§eme minutiga.\\nOllie: Ok.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][4]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7526f16-c78d-4808-9c82-1afc7f1c62a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jane on Varssavis. Ollie ja Jane korraldavad peo. Jane kaotas oma kalendri. Nad saavad sel n√§dalal reedel l√µunat. Ollie helistas kogemata Jane'ile ja r√§√§kis viskist. Jane t√ºhistab l√µunas√∂√∂gi. Nad kohtuvad kell 18 teeks.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][4]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be2a0b77-2fd1-4d90-a85d-b0217c8164e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies have shown that owning a dog is good.\n"
     ]
    }
   ],
   "source": [
    "summarize(\"studies have shown that owning a dog is good for you\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da38e6f1-0256-4072-9372-4233844c8397",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koonderakond kaotas m√§rtsis m√§rtsis peaministripartei toetust. Koonderakonna toetus on n√º√ºd 20,6 protsenti. Koonderakonna toetus on n√º√ºd 20,7 protsenti.\n"
     ]
    }
   ],
   "source": [
    "input_text=\"\"\"Veel veebruaris oli Soome k√µige populaarsem partei Koonderakond, kuid kaotas m√§rtsis selle tiitli SDP-le. M√§rtsis langes peaministripartei  toetus √ºhe protsendi v√µrra ning Koonderakonna toetus on n√º√ºd 20,6 protsenti.\n",
    "SDP suurendas toetust naiste ja noorte hulgas. M√§rtsis t√µusis SDP toetus 1,9 protsenti ning erakonna toetus on n√º√ºd 21,7 protsenti. \n",
    "Koonderakonna kannul on P√µlissoomlased, rahandusminister Riikka Purra kodupartei toetus on 17,4 protsenti.\"\"\"\n",
    "summarize(input_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1aa7c8-7ca6-4e3e-bced-96aeff1cb453",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}