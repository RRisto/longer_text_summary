{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30935c25-5c96-476f-a0b0-b52de2cab591",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/risto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import GenerationConfig\n",
    "from random import randrange\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3ccec9-1a90-4e52-93df-277c06e0dedd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e69b14-4a03-463e-b71b-79c6b800edc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf42950-9405-4d2a-bb56-46991f67c626",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c325f1-27e3-46af-9918-72289c66fad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad355d6-5bea-4ecf-9a4e-41e1355fe940",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f56bd08-ae69-449c-8f1f-8705921014fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a35e3-6189-4778-a87b-9f5ec059b1a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "source: https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6685f2-29ac-4018-9486-b33f823f014a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df83dd6-353c-4e01-874b-1fb76d377560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "model_id=\"google/mt5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd92d3c-0e82-4063-b989-e0baa11c4fa6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4266064454395085\n",
      "0.4266064454395085\n"
     ]
    }
   ],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "print(msize(model.shared) / msize(model))   \n",
    "print(msize(model.lm_head) / msize(model))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16672a47-8e1b-4318-bf3f-6ec962315d35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "about 42% are embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2ca13-1507-402d-bb77-289d594c6219",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0488a3e2-a550-4014-9f58-25686dd78a8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"TalTechNLP/samsum_ee\"\n",
    "\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0af9bcd-5210-42f7-ba0a-08d30b42886a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 13199\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2ad20f-2136-4d1e-af02-960588efe7a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 809\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset['test'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff097111-2531-4ce9-ab97-510ef699fa4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## update model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3658a305-6512-4b78-a6ee-64fdccf3780d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26398"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts4vocab=train_dataset['dialogue']+train_dataset['summary']\n",
    "len(texts4vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19cfcdd-c6da-4ab7-812a-5bd2c09efa86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22815 0.0912235105957617\n"
     ]
    }
   ],
   "source": [
    "cnt_et = Counter()\n",
    "for text in texts4vocab:\n",
    "    cnt_et.update(tokenizer.encode(text))\n",
    "print(len(cnt_et), len(cnt_et)/tokenizer.vocab_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590004ae-8bd8-471b-a3df-f4e4ced08f35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(259, 222378),\n",
       " (267, 152951),\n",
       " (260, 109486),\n",
       " (261, 103717),\n",
       " (351, 37623),\n",
       " (291, 35910),\n",
       " (262, 29563),\n",
       " (1, 26398),\n",
       " (263, 24319),\n",
       " (432, 24040),\n",
       " (265, 23799),\n",
       " (309, 21963),\n",
       " (266, 21379),\n",
       " (383, 20708),\n",
       " (327, 19689),\n",
       " (316, 18386),\n",
       " (1055, 17737),\n",
       " (496, 17714),\n",
       " (1221, 15946),\n",
       " (178005, 14920)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_et.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b33882-e523-47fb-82d6-882dde21115b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23422\n"
     ]
    }
   ],
   "source": [
    "new_tokens = set(range(1000))\n",
    "for i, (k, v) in enumerate(cnt_et.items()):\n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "for t in range(tokenizer.vocab_size - 100, tokenizer.vocab_size):\n",
    "    new_tokens.add(t)\n",
    "print(len(new_tokens))\n",
    "kept_ids = sorted(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed5e901-6a7d-44d4-a9e7-b3616499d6fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[250090,\n",
       " 250091,\n",
       " 250092,\n",
       " 250093,\n",
       " 250094,\n",
       " 250095,\n",
       " 250096,\n",
       " 250097,\n",
       " 250098,\n",
       " 250099]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_ids[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808335c-9428-4102-bb58-db3e7d214081",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### update model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4c275e-2b32-46ee-ac7a-d06a6ef72f3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_size = len(kept_ids)\n",
    "new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)\n",
    "new_head = torch.nn.Linear(in_features=model.lm_head.in_features, out_features=new_size, bias=False)\n",
    "for new_id, old_id in enumerate(kept_ids):\n",
    "    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]\n",
    "    new_head.weight.data[new_id] = model.lm_head.weight.data[old_id]\n",
    "model.shared.weight = new_emb.weight\n",
    "model.lm_head.weight = new_head.weight\n",
    "model.config.__dict__['vocab_size'] = new_size\n",
    "model.config.__dict__['_name_or_path'] = 'mt5_et/mt_et_t5-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f2fad-46bb-49d6-a79e-12713f02d131",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### update tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "400f729c-00f5-47aa-a2b2-ae51c67a1866",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"
     ]
    }
   ],
   "source": [
    "!apt-get update \n",
    "!apt install protobuf-compiler -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03c511e8-c156-4f7a-8bcb-0719fa8610d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-10 06:47:01--  https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14023 (14K) [text/plain]\n",
      "Saving to: ‘sentencepiece_model.proto.3’\n",
      "\n",
      "sentencepiece_model 100%[===================>]  13.69K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-04-10 06:47:01 (20.3 MB/s) - ‘sentencepiece_model.proto.3’ saved [14023/14023]\n",
      "\n",
      "the loaded model has pieces: 250100\n",
      "the new pieces: 23422\n",
      "23422\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto\n",
    "! protoc --python_out=. sentencepiece_model.proto\n",
    "import sentencepiece.sentencepiece_model_pb2 as spmp\n",
    "smp = tokenizer.sp_model.serialized_model_proto()\n",
    "m = spmp.ModelProto()\n",
    "m.ParseFromString(smp)\n",
    "print('the loaded model has pieces:', len(m.pieces))\n",
    "new_pieces = [m.pieces[idx] for idx in kept_ids]\n",
    "print('the new pieces:', len(new_pieces))\n",
    "# replace the content of the first 30K pieces\n",
    "for i, p in enumerate(new_pieces):\n",
    "    m.pieces[i].piece = p.piece\n",
    "    m.pieces[i].score = p.score\n",
    "    m.pieces[i].type = p.type\n",
    "# drop the remaining pieces\n",
    "n = len(new_pieces)\n",
    "for i in range(len(m.pieces) - n):\n",
    "    m.pieces.pop(len(m.pieces) - 1)\n",
    "print(len(m.pieces))\n",
    "with open('mt5_et/new_sp.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())\n",
    "new_tokenizer = T5Tokenizer('mt5_et/new_sp.model', extra_ids=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0af3370-804f-412b-ab64-d1f671357f54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained('mt5_et')\n",
    "model.save_pretrained('mt5_et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ca62b8f-37bf-4876-9a31-a3c7752f7701",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('mt5_et')\n",
    "model = T5ForConditionalGeneration.from_pretrained('mt5_et', max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c1b3b-d201-49bf-9362-85cfc93c3366",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## prep data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58462ab6-a200-4870-9404-116e6e9dd483",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_source_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f000e83a-938f-4062-bbd9-81e49581f3bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_target_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3737edea-df3e-4261-9141-ad2c8368fbd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23785d0a676343b48ae186557665d5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780e8218947c479fa2ad70a12cafbeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/809 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc810e-4647-46f0-8813-a0ac8f5f5a9e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de471419-c631-41db-9055-485596a606d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bcf0960-da63-4f4f-a88b-6bd8cdce783d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c1268-ae7d-4537-9de0-f6b448a5af76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9394d21f-92d3-42fa-8597-a2ff3c8aad64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStoppingCallback(3, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c4ae189-8f5e-4add-8b34-4cf0c40c2ce6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"#for some reason this was not working\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=30,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    "    # generation_max_length=40\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "027c86c4-b33e-429c-8a8a-e9d975162bcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49500' max='49500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49500/49500 6:51:57, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.613500</td>\n",
       "      <td>2.712732</td>\n",
       "      <td>25.906500</td>\n",
       "      <td>8.160100</td>\n",
       "      <td>22.662800</td>\n",
       "      <td>24.040100</td>\n",
       "      <td>18.700865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.224900</td>\n",
       "      <td>2.552037</td>\n",
       "      <td>28.076700</td>\n",
       "      <td>9.282900</td>\n",
       "      <td>24.455800</td>\n",
       "      <td>26.048200</td>\n",
       "      <td>23.902349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.010400</td>\n",
       "      <td>2.460055</td>\n",
       "      <td>29.156700</td>\n",
       "      <td>10.205600</td>\n",
       "      <td>25.723000</td>\n",
       "      <td>27.350600</td>\n",
       "      <td>29.870210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.873300</td>\n",
       "      <td>2.387637</td>\n",
       "      <td>30.111000</td>\n",
       "      <td>10.863300</td>\n",
       "      <td>26.417800</td>\n",
       "      <td>28.336500</td>\n",
       "      <td>29.226205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.768400</td>\n",
       "      <td>2.346879</td>\n",
       "      <td>29.914600</td>\n",
       "      <td>11.152300</td>\n",
       "      <td>26.280800</td>\n",
       "      <td>28.139400</td>\n",
       "      <td>25.719407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.698200</td>\n",
       "      <td>2.318779</td>\n",
       "      <td>31.321600</td>\n",
       "      <td>11.811700</td>\n",
       "      <td>27.484300</td>\n",
       "      <td>29.489300</td>\n",
       "      <td>25.085290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.615400</td>\n",
       "      <td>2.297585</td>\n",
       "      <td>30.961700</td>\n",
       "      <td>11.680500</td>\n",
       "      <td>27.162100</td>\n",
       "      <td>29.244900</td>\n",
       "      <td>28.719407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.555300</td>\n",
       "      <td>2.264596</td>\n",
       "      <td>31.558100</td>\n",
       "      <td>12.171000</td>\n",
       "      <td>27.651200</td>\n",
       "      <td>29.648100</td>\n",
       "      <td>25.922126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.528400</td>\n",
       "      <td>2.245093</td>\n",
       "      <td>32.059300</td>\n",
       "      <td>12.613200</td>\n",
       "      <td>27.815100</td>\n",
       "      <td>30.009100</td>\n",
       "      <td>28.139679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.477300</td>\n",
       "      <td>2.239874</td>\n",
       "      <td>31.788800</td>\n",
       "      <td>12.324000</td>\n",
       "      <td>27.475800</td>\n",
       "      <td>29.657500</td>\n",
       "      <td>26.331273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.430200</td>\n",
       "      <td>2.214731</td>\n",
       "      <td>32.095500</td>\n",
       "      <td>12.488900</td>\n",
       "      <td>27.741100</td>\n",
       "      <td>30.119300</td>\n",
       "      <td>27.473424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.412100</td>\n",
       "      <td>2.206002</td>\n",
       "      <td>32.120500</td>\n",
       "      <td>12.448300</td>\n",
       "      <td>27.742900</td>\n",
       "      <td>30.178400</td>\n",
       "      <td>29.410383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.356300</td>\n",
       "      <td>2.190853</td>\n",
       "      <td>32.792400</td>\n",
       "      <td>12.645900</td>\n",
       "      <td>28.218100</td>\n",
       "      <td>30.645700</td>\n",
       "      <td>26.954265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.341000</td>\n",
       "      <td>2.188806</td>\n",
       "      <td>33.112300</td>\n",
       "      <td>12.803300</td>\n",
       "      <td>28.387900</td>\n",
       "      <td>30.941400</td>\n",
       "      <td>29.767614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.330600</td>\n",
       "      <td>2.183685</td>\n",
       "      <td>32.745500</td>\n",
       "      <td>12.908800</td>\n",
       "      <td>28.203100</td>\n",
       "      <td>30.661300</td>\n",
       "      <td>29.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.295800</td>\n",
       "      <td>2.168080</td>\n",
       "      <td>33.042600</td>\n",
       "      <td>12.923700</td>\n",
       "      <td>28.384600</td>\n",
       "      <td>30.890700</td>\n",
       "      <td>29.665019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.275000</td>\n",
       "      <td>2.160700</td>\n",
       "      <td>33.082700</td>\n",
       "      <td>12.694300</td>\n",
       "      <td>28.517000</td>\n",
       "      <td>30.910500</td>\n",
       "      <td>29.825711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.253600</td>\n",
       "      <td>2.169038</td>\n",
       "      <td>33.057700</td>\n",
       "      <td>13.040100</td>\n",
       "      <td>28.656900</td>\n",
       "      <td>30.970400</td>\n",
       "      <td>28.840544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.232100</td>\n",
       "      <td>2.159625</td>\n",
       "      <td>33.239100</td>\n",
       "      <td>13.124900</td>\n",
       "      <td>28.665500</td>\n",
       "      <td>31.138200</td>\n",
       "      <td>29.196539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.212400</td>\n",
       "      <td>2.159690</td>\n",
       "      <td>33.066000</td>\n",
       "      <td>13.017100</td>\n",
       "      <td>28.614000</td>\n",
       "      <td>30.920600</td>\n",
       "      <td>30.682324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.189800</td>\n",
       "      <td>2.149160</td>\n",
       "      <td>33.369900</td>\n",
       "      <td>13.137100</td>\n",
       "      <td>28.925600</td>\n",
       "      <td>31.347100</td>\n",
       "      <td>29.855377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.193800</td>\n",
       "      <td>2.147640</td>\n",
       "      <td>32.944300</td>\n",
       "      <td>12.894500</td>\n",
       "      <td>28.500400</td>\n",
       "      <td>30.953700</td>\n",
       "      <td>30.948084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.163600</td>\n",
       "      <td>2.148372</td>\n",
       "      <td>33.446200</td>\n",
       "      <td>13.208200</td>\n",
       "      <td>28.777600</td>\n",
       "      <td>31.356300</td>\n",
       "      <td>31.180470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.166200</td>\n",
       "      <td>2.146236</td>\n",
       "      <td>33.570400</td>\n",
       "      <td>13.102200</td>\n",
       "      <td>28.879500</td>\n",
       "      <td>31.495600</td>\n",
       "      <td>29.144623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.164500</td>\n",
       "      <td>2.134749</td>\n",
       "      <td>33.779800</td>\n",
       "      <td>13.402400</td>\n",
       "      <td>29.134700</td>\n",
       "      <td>31.684400</td>\n",
       "      <td>28.945612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.158500</td>\n",
       "      <td>2.137822</td>\n",
       "      <td>33.546200</td>\n",
       "      <td>13.151700</td>\n",
       "      <td>28.999100</td>\n",
       "      <td>31.455000</td>\n",
       "      <td>29.046972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.159100</td>\n",
       "      <td>2.137201</td>\n",
       "      <td>33.497800</td>\n",
       "      <td>13.180300</td>\n",
       "      <td>28.840200</td>\n",
       "      <td>31.375100</td>\n",
       "      <td>29.630408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.142700</td>\n",
       "      <td>2.134053</td>\n",
       "      <td>33.628500</td>\n",
       "      <td>13.182600</td>\n",
       "      <td>28.892500</td>\n",
       "      <td>31.443900</td>\n",
       "      <td>29.482077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.142100</td>\n",
       "      <td>2.135680</td>\n",
       "      <td>33.472700</td>\n",
       "      <td>13.177600</td>\n",
       "      <td>28.794100</td>\n",
       "      <td>31.378900</td>\n",
       "      <td>30.009889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.149900</td>\n",
       "      <td>2.135469</td>\n",
       "      <td>33.771200</td>\n",
       "      <td>13.379200</td>\n",
       "      <td>29.035700</td>\n",
       "      <td>31.579500</td>\n",
       "      <td>29.080346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49500, training_loss=2.4860098420730745, metrics={'train_runtime': 24718.4609, 'train_samples_per_second': 16.019, 'train_steps_per_second': 2.003, 'total_flos': 1.363698115411968e+17, 'train_loss': 2.4860098420730745, 'epoch': 30.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa402a3-35c1-48bb-9e33-c09ba525d4c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825c6dbc-ae7d-426d-96e4-9e8d14376e8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_id='mt5-small-TalTechNLP/samsum_ee/checkpoint-46200/'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfeea40b-7be7-42d5-8d77-3f92872cd9f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model=model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07061c04-ac02-42fe-8bd9-c0a588dcf9bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_texts_labels_metrics(texts, summaries, model, tokenizer, max_input_length=1024, max_output_length=512, batch_size = 10, temperature=0):\n",
    "    true_labels=tokenizer(\n",
    "        summaries, return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids.cpu()\n",
    "    input_ids = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids  \n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(0, input_ids.size(0), batch_size):\n",
    "        batch_input_ids = input_ids[i:i + batch_size].to('cuda')\n",
    "        batch_outputs = model.generate(input_ids=batch_input_ids, max_length=max_output_length, temperature=temperature)\n",
    "        predictions.extend(batch_outputs.cpu().detach().numpy())\n",
    "    max_length = max(len(p) for p in predictions)\n",
    "    padded_predictions = [np.pad(p, (0, max_length - len(p)), mode='constant') for p in predictions]\n",
    "    outputs = torch.tensor(padded_predictions)\n",
    "    eval_preds = (outputs, true_labels.cpu())\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1512cb-4013-4755-ae87-70a0e4d6f1ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_40602/2151791639.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  outputs = torch.tensor(padded_predictions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 32.7898,\n",
       " 'rouge2': 13.0878,\n",
       " 'rougeL': 28.3526,\n",
       " 'rougeLsum': 30.7215,\n",
       " 'gen_len': 31.597033374536466}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics=calc_texts_labels_metrics(test_dataset['dialogue'], test_dataset['summary'], model, tokenizer)\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c751f-8c86-4371-b961-994df10e6767",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a53e1a-8ee0-4450-8f90-794a997e9e27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def summarize(text, model, tokenizer, max_input_length=1024, max_new_tokens=512):\n",
    "    input_ids = tokenizer(\n",
    "         text, return_tensors=\"pt\",\n",
    "        max_length=max_input_length\n",
    "    ).input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids=input_ids.to('cuda'), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f305dd4a-1c05-490e-a936-3969ea516119",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda saadab Betty telefoninumbri, et saada talle sõnumi. Betty helistas talle viimati, kui nad koos pargis olisid.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][0]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a603bd09-3b4e-47e8-bf78-2a8f6ccaf6ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah: Hei, kas sul on Betty number?\\nAmanda: Vaatan järele.\\nHannah: <file_gif>\\nAmanda: Vabandust, ei leia seda.\\nAmanda: Küsi Larrylt.\\nAmanda: Ta helistas talle viimati, kui me koos pargis olime.\\nHannah: Ma ei tunne teda hästi.\\nHannah: <file_gif>\\nAmanda: Ära ole häbelik, ta on väga tore.\\nHannah: Kui sa ütled nii..\\nHannah: Ma eelistaksin, et sa talle sõnumi saadaksid.\\nAmanda: Lihtsalt saada talle sõnum 🙂\\nHannah: Urgh.. Olgu siis\\nHannah: Nägemist\\nAmanda: Nägemist-nägemist'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beee675b-a0f3-4943-9951-ee948415a24e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah vajab Betty telefoninumbrit, kuid Amandal seda pole. Ta peab Larryga ühendust võtma.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2065e610-7af4-4e53-89a6-87bd119c1884",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eric ja Rob vaatavad rongi osa. Rob vaatab rongi osa Youtube'is.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][1]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3ff60ed-775b-40af-85fa-dcd1dfeea31e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric: MASIN!\\nRob: See on nii lahe!\\nEric: Ma tean! Ja näitab, kuidas ameeriklased näevad venelasi ;)\\nRob: Ja see on tõesti naljakas!\\nEric: Ma tean! Mulle meeldib eriti rongi osa!\\nRob: Hahaha! Keegi ei räägi masinaga nii!\\nEric: Kas see on tema ainus etendus?\\nRob: Ei tea. Ma vaatan järele.\\nEric: Kindlasti.\\nRob: Selgub, et ei! Mõned tema etendused on Youtube'is.\\nEric: Lahe! Ma vaatan neid kohe!\\nRob: Mina ka!\\nEric: MASIN!\\nRob: MASIN!\\nEric: Kuni kohtumiseni?\\nRob: Muidugi :)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][1]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb43273-7c09-4538-a2c0-bfa5a645eb49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric ja Rob lähevad vaatama stand-up'i YouTube'is.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][1]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b5c607-6e64-41b4-ae57-bd14dc9b5966",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenny soovib osta esimese või kolmanda paari. Bob soovitab talle erinevaid riietumisvõimalusi.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][2]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a95fc7-be0c-4cee-bb99-2a3ab7270c0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lenny: Kallis, kas sa saaksid mulle millegagi abi anda?\\nBob: Muidugi, mis toimub?\\nLenny: Millise ma peaksin valima?\\nBob: Saada mulle pilte.\\nLenny: <file_photo>\\nLenny: <file_photo>\\nLenny: <file_photo>\\nBob: Mulle meeldivad kõige rohkem esimesed.\\nLenny: Aga mul on juba lillad püksid. Kas on mõtet omada kahte paari?\\nBob: Mul on neli musta paari :D :D\\nLenny: Jah, aga kas ma ei peaks valima erinevat värvi?\\nBob: Oluline on see, milline annab sulle kõige rohkem riietumisvõimalusi.\\nLenny: Nii et ma arvan, et ostan esimese või kolmanda paari siis.\\nBob: Vali parim kvaliteet siis.\\nLenny: Sul on õigus, aitäh.\\nBob: Pole probleemi :)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][2]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d79f947-cf55-4064-9eb8-504ea997bdd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lenny ei suuda otsustada, millised püksid osta. Bob nõustas Lennyt selles küsimuses. Lenny läheb Bobi nõu järgi ja valib kvaliteetseimad püksid.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][2]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a06f293e-aa4c-420f-a9d5-e12db54de64d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma on mures toiduvalmistamise pärast. Will jõuab varsti koju, kuna tal pole isu.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][3]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcad0866-0c6d-47e1-9d58-489db0f887d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Will: hei kallis, mida sa täna õhtuks süüa tahad?\\nEmma: oh, ära täna üldse muretse selle pärast\\nWill: mida sa selle all mõtled? kas kõik on korras?\\nEmma: mitte päris, aga see on ok, ära muretse toiduvalmistamise pärast, mul pole isu\\nWill: millal sa koju jõuad?\\nEmma: varsti, loodetavasti\\nWill: kas sa oled kindel? võib-olla tahad, et ma sind järele tuleksin?\\nEmma: ei ei, see on korras. Ma jõuan varsti koju, ma ütlen sulle, kui ma koju jõuan.\\nWill: Selge, armastan sind.\\nEmma: armastan sind ka.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][3]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a5768b-4b69-4bad-9e2f-69def9a77906",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma tuleb varsti koju ja ta annab Willile teada.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][3]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d04131b-6ea2-4d9c-ae16-8e1d2789166d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane ja Ollie lähevad reedel Varssavis lõunaks. Nad kohtuvad reedel kell 18.00 Marokosse.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][4]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2563d342-cb99-4ab3-9a71-b0ed2dd695f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ollie: Tere, kas sa oled Varssavis?\\nJane: Jah, just tagasi! Muide, kas sa oled vaba õhtusöögiks 19. kuupäeval?\\nOllie: Ei ole!\\nJane: Aga 18.?\\nOllie: Ei, meil on see pidu ja sa pead seal olema, mäletad?\\nJane: Oh õige! Ma kaotasin oma kalendri.. aitäh, et meelde tuletasid.\\nOllie: Kas me lõunatame sel nädalal?\\nJane: Hea meelega!\\nOllie: Reede?\\nJane: Ok.\\nJane: Mida sa mõtled \"meil pole enam viskit!\" lol..\\nOllie: Mida!!!?\\nJane: Sa helistasid mulle ja ainus asi, mida ma kuulsin, oli see lause viski kohta... mis sinuga lahti on?\\nOllie: Oh oh... väga imelik! Pean olema ettevaatlik, võib-olla on minu telefonis mõni spioon! lol\\nJane: Ära muretse, me kontrollime reedel.\\nOllie: Ära unusta päikest kaasa võtta.\\nJane: Ma ei jõua ära oodata, et Marokosse jõuda..\\nOllie: Naudi ja näeme reedel.\\nJane: Vabandust, Ollie, mul on väga kiire, mul ei ole homme lõunaks aega, aga ehk kell 18 pärast minu kursusi? See Maroko reis oli nii tore, aga aeganõudev!\\nOllie: Ok, teeks siis teed!\\nJane: Olen teel..\\nOllie: Tee on valmis, kas tõid küpsetised kaasa?\\nJane: Ma sõin need juba ära... näeme minutiga.\\nOllie: Ok.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][4]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7526f16-c78d-4808-9c82-1afc7f1c62a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jane on Varssavis. Ollie ja Jane korraldavad peo. Jane kaotas oma kalendri. Nad saavad sel nädalal reedel lõunat. Ollie helistas kogemata Jane'ile ja rääkis viskist. Jane tühistab lõunasöögi. Nad kohtuvad kell 18 teeks.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][4]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be2a0b77-2fd1-4d90-a85d-b0217c8164e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies have shown that owning a dog is good.\n"
     ]
    }
   ],
   "source": [
    "summarize(\"studies have shown that owning a dog is good for you\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da38e6f1-0256-4072-9372-4233844c8397",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koonderakond kaotas märtsis märtsis peaministripartei toetust. Koonderakonna toetus on nüüd 20,6 protsenti. Koonderakonna toetus on nüüd 20,7 protsenti.\n"
     ]
    }
   ],
   "source": [
    "input_text=\"\"\"Veel veebruaris oli Soome kõige populaarsem partei Koonderakond, kuid kaotas märtsis selle tiitli SDP-le. Märtsis langes peaministripartei  toetus ühe protsendi võrra ning Koonderakonna toetus on nüüd 20,6 protsenti.\n",
    "SDP suurendas toetust naiste ja noorte hulgas. Märtsis tõusis SDP toetus 1,9 protsenti ning erakonna toetus on nüüd 21,7 protsenti. \n",
    "Koonderakonna kannul on Põlissoomlased, rahandusminister Riikka Purra kodupartei toetus on 17,4 protsenti.\"\"\"\n",
    "summarize(input_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1aa7c8-7ca6-4e3e-bced-96aeff1cb453",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}