{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde883fd-1389-47af-a85d-c9eb670dcd9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:878: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /home/risto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback, MBartConfig\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizerFast,MBartTokenizer,Trainer, TrainingArguments\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from hftrim.TokenizerTrimmer import TokenizerTrimmer\n",
    "from hftrim.ModelTrimmers import MBartTrimmer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "# from transformers.convert_slow_tokenizer import convert_slow_tokenizer\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from random import randrange\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from lsg_converter import LSGConverter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ea95d5-17ce-4915-b20a-c2119e3c6892",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902452d9-f3c2-4085-8fa6-5c7eaf94ea14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ccec9-1a90-4e52-93df-277c06e0dedd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e69b14-4a03-463e-b71b-79c6b800edc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf42950-9405-4d2a-bb56-46991f67c626",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c325f1-27e3-46af-9918-72289c66fad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9758d323-6b9d-4bdf-9ae1-19aaebb450a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6685f2-29ac-4018-9486-b33f823f014a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef187b1-9e0e-44ea-bf56-2c1b114dc6cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_id=\"facebook/mbart-large-cc25\"\n",
    "src_lang = \"et_EE\" # Example source language code\n",
    "tgt_lang = \"et_EE\" # Example target language code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df83dd6-353c-4e01-874b-1fb76d377560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_slow = MBartTokenizer.from_pretrained(model_id)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fe507c-b7b8-48ca-b7c5-3a5fbf57ab38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Important: Set both source and target languages\n",
    "tokenizer.src_lang = src_lang\n",
    "tokenizer.tgt_lang = tgt_lang\n",
    "\n",
    "tokenizer_slow.src_lang = src_lang\n",
    "tokenizer_slow.tgt_lang = tgt_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e0b57c-46de-424e-81a8-e67640e0d45e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41913215486098887\n",
      "0.41913215486098887\n"
     ]
    }
   ],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "#share of params in embedding\n",
    "print(msize(model.model.shared) / msize(model))   \n",
    "print(msize(model.lm_head) / msize(model))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2ca13-1507-402d-bb77-289d594c6219",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0488a3e2-a550-4014-9f58-25686dd78a8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"TalTechNLP/samsum_ee\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed5008b-5135-4442-9de1-565471151878",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train']=dataset['train'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "dataset['test'] = dataset['test'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607554e0-274a-417b-a5d9-12f19d42e9af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 13199\n",
      "Test dataset size: 809\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "841231ad-5ce9-40ee-a268-acf4e5201989",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 13199\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5124a695-a6c7-4825-bce6-681575fcd281",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 809\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset['test'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10c473-40b5-4f9a-825d-ecb94eeef887",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## trim models vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33d3e93b-68bc-4acf-b5c9-aa9978e2e734",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26398"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts4vocab=train_dataset['dialogue']+train_dataset['summary']\n",
    "len(texts4vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f78824c-4d9e-4121-bcfa-0f08e6d3beec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = MBartConfig.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13803b0c-fbe6-49c5-bd71-914a36224c0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1981 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250000/250000 [00:00<00:00, 1167225.66it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# trim tokenizer\n",
    "tokenizer_slow_trim = TokenizerTrimmer(tokenizer_slow)\n",
    "tokenizer_slow_trim.make_vocab(texts4vocab)\n",
    "tokenizer_slow_trim.make_tokenizer()\n",
    "\n",
    "# trim model\n",
    "model_trim = MBartTrimmer(model, config, tokenizer_slow_trim.trimmed_tokenizer)\n",
    "model_trim.make_weights(tokenizer_slow_trim.trimmed_vocab_ids)\n",
    "model_trim.make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715da82-494a-4ee3-b30f-14da9183aab0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save trimmed model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11147006-b556-4bd6-8317-a7d41556213c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_slow_trim.trimmed_tokenizer.save_pretrained('mbart-large-cc25_et')\n",
    "model_trim.trimmed_model.save_pretrained('mbart-large-cc25_et')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9eca1-6156-4581-a51e-0271b102f266",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e858cc4a-d4cc-4506-90be-8e7b825c0383",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = MBartTokenizer.from_pretrained('mbart-large-cc25_et')\n",
    "tokenizer = MBartTokenizerFast.from_pretrained(\"mbart-large-cc25_et/\", from_slow=True)\n",
    "tokenizer.src_lang = src_lang\n",
    "tokenizer.tgt_lang = tgt_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c67d1da-805a-4a30-a6da-cbee306fb1c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained('mbart-large-cc25_et', max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b421bdd-e71f-4f62-b83a-a0bc1a7ac1c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model and make it accepting longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20490531-b219-4b41-92cb-6553ea6e0fe1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1067: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/risto/.local/lib/python3.9/site-packages/transformers/configuration_utils.py:508: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/home/risto/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2759: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of LSGMBartForConditionalGeneration were not initialized from the model checkpoint at mbart-large-cc25_et and are newly initialized: ['model.encoder.global_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/risto/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lsg_converter.mbart.modeling_lsg_mbart.LSGMBartForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "converter = LSGConverter(max_sequence_length=2048)\n",
    "\n",
    "model_id='mbart-large-cc25_et'\n",
    "model, tokenizer = converter.convert_from_pretrained(model_id, num_global_tokens=7)\n",
    "tokenizer.src_lang = src_lang\n",
    "tokenizer.tgt_lang = tgt_lang\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b298d1ff-992e-4a20-99c4-c3aae20fb8c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('mbart-large-cc25_lsg_2048_et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2def0ad-aa26-41aa-bdc3-2ffefedb6bab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/tokenizer_config.json',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/special_tokens_map.json',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/sentencepiece.bpe.model',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/added_tokens.json',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('mbart-large-cc25_lsg_2048_et')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c1b3b-d201-49bf-9362-85cfc93c3366",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## prep data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58462ab6-a200-4870-9404-116e6e9dd483",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_source_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f000e83a-938f-4062-bbd9-81e49581f3bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_target_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3737edea-df3e-4261-9141-ad2c8368fbd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d4b93f9-c7fd-4683-b21d-15283e217b7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f766165-4f45-40a8-9898-c4a458584f01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc810e-4647-46f0-8813-a0ac8f5f5a9e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de471419-c631-41db-9055-485596a606d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bcf0960-da63-4f4f-a88b-6bd8cdce783d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c1268-ae7d-4537-9de0-f6b448a5af76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9394d21f-92d3-42fa-8597-a2ff3c8aad64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStoppingCallback(3, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c4ae189-8f5e-4add-8b34-4cf0c40c2ce6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face repository id\n",
    "#save to other disc where chmod is supported\n",
    "repository_id = f\"mbart-large-cc25_lsg_2048_et_TalTechNLP/{dataset_id}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027c86c4-b33e-429c-8a8a-e9d975162bcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MBartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16500' max='33000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16500/33000 5:04:37 < 5:04:39, 0.90 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.069600</td>\n",
       "      <td>1.933898</td>\n",
       "      <td>36.339800</td>\n",
       "      <td>15.320700</td>\n",
       "      <td>30.846800</td>\n",
       "      <td>33.451100</td>\n",
       "      <td>29.572311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.522100</td>\n",
       "      <td>1.894038</td>\n",
       "      <td>37.566600</td>\n",
       "      <td>15.305900</td>\n",
       "      <td>31.708200</td>\n",
       "      <td>34.706200</td>\n",
       "      <td>26.925834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.085500</td>\n",
       "      <td>1.983595</td>\n",
       "      <td>37.673800</td>\n",
       "      <td>16.121600</td>\n",
       "      <td>32.103900</td>\n",
       "      <td>34.945300</td>\n",
       "      <td>25.632880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>2.337484</td>\n",
       "      <td>34.909000</td>\n",
       "      <td>14.002000</td>\n",
       "      <td>28.889000</td>\n",
       "      <td>32.182400</td>\n",
       "      <td>72.166873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.499300</td>\n",
       "      <td>2.539751</td>\n",
       "      <td>37.482400</td>\n",
       "      <td>15.062900</td>\n",
       "      <td>31.021300</td>\n",
       "      <td>34.480600</td>\n",
       "      <td>31.426452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16500, training_loss=1.2926952958540483, metrics={'train_runtime': 18278.2764, 'train_samples_per_second': 7.221, 'train_steps_per_second': 1.805, 'total_flos': 2.8045274990444544e+17, 'train_loss': 1.2926952958540483, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1459ff5-8265-4fab-a1f6-5a321128e221",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calc metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1dfb433-b776-4c97-b4cf-47c92b1243c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_id=\"mbart-large-cc25_lsg_2048_et_TalTechNLP/TalTechNLP/samsum_ee/checkpoint-6600/\"\n",
    "tokenizer = MBartTokenizerFast.from_pretrained('transformers/mbart-large-cc25_lsg_2048_et',\n",
    "                                               from_slow=True)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8116a954-a542-4638-a974-fa6bf94089ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model=model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f9f6b0-1ff8-485a-b16c-06543479dc88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443647/343644409.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  outputs = torch.tensor(padded_predictions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 37.0547,\n",
       " 'rouge2': 14.8011,\n",
       " 'rougeL': 30.5113,\n",
       " 'rougeLsum': 34.1101,\n",
       " 'gen_len': 456.85784919653895}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_texts_labels_metrics(texts, summaries, model, tokenizer, max_input_length=1024, max_output_length=512, batch_size = 10):\n",
    "    true_labels=tokenizer(\n",
    "        summaries, return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids.cpu()\n",
    "    input_ids = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids  \n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(0, input_ids.size(0), batch_size):\n",
    "        batch_input_ids = input_ids[i:i + batch_size].to('cuda')\n",
    "        batch_outputs = model.generate(input_ids=batch_input_ids, max_length=max_output_length)\n",
    "        predictions.extend(batch_outputs.cpu().detach().numpy())\n",
    "    max_length = max(len(p) for p in predictions)\n",
    "    padded_predictions = [np.pad(p, (0, max_length - len(p)), mode='constant') for p in predictions]\n",
    "    outputs = torch.tensor(padded_predictions)\n",
    "    eval_preds = (outputs, true_labels.cpu())\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    return metrics\n",
    "\n",
    "eval_metrics=calc_texts_labels_metrics(dataset['test']['dialogue'], dataset['test']['summary'], model, tokenizer)\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c751f-8c86-4371-b961-994df10e6767",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f99c6f-f858-4f42-baa1-7e0196bd9c98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda ei leia Betty telefoninumbrit. Hannah saadab talle s√µnumi Larrylt. Larry helistas talle viimati, kui nad koos pargis oliid. Hannah ei tunne teda h√§sti.\n"
     ]
    }
   ],
   "source": [
    "def summarize(text, model, tokenizer, max_input_length=2048, max_new_tokens=512):\n",
    "    input_ids = tokenizer(\n",
    "         text, return_tensors=\"pt\",\n",
    "        max_length=max_input_length\n",
    "    ).input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids=input_ids.to('cuda'), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    \n",
    "summarize(dataset['test'][0]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e48a4fc-feb7-4307-9bf0-5f7449493932",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah: Hei, kas sul on Betty number?\\nAmanda: Vaatan j√§rele.\\nHannah: <file_gif>\\nAmanda: Vabandust, ei leia seda.\\nAmanda: K√ºsi Larrylt.\\nAmanda: Ta helistas talle viimati, kui me koos pargis olime.\\nHannah: Ma ei tunne teda h√§sti.\\nHannah: <file_gif>\\nAmanda: √Ñra ole h√§belik, ta on v√§ga tore.\\nHannah: Kui sa √ºtled nii..\\nHannah: Ma eelistaksin, et sa talle s√µnumi saadaksid.\\nAmanda: Lihtsalt saada talle s√µnum üôÇ\\nHannah: Urgh.. Olgu siis\\nHannah: N√§gemist\\nAmanda: N√§gemist-n√§gemist'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dataset['test'][0]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aabbb9e4-5dcc-4c8f-9035-ccec584f1249",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rob ja Eric vaatavad Masin'i. Nad kohtuvad siis.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][1]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd4fd15c-30c8-4ade-9f55-20687b6f3ec4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric: MASIN!\\nRob: See on nii lahe!\\nEric: Ma tean! Ja n√§itab, kuidas ameeriklased n√§evad venelasi ;)\\nRob: Ja see on t√µesti naljakas!\\nEric: Ma tean! Mulle meeldib eriti rongi osa!\\nRob: Hahaha! Keegi ei r√§√§gi masinaga nii!\\nEric: Kas see on tema ainus etendus?\\nRob: Ei tea. Ma vaatan j√§rele.\\nEric: Kindlasti.\\nRob: Selgub, et ei! M√µned tema etendused on Youtube'is.\\nEric: Lahe! Ma vaatan neid kohe!\\nRob: Mina ka!\\nEric: MASIN!\\nRob: MASIN!\\nEric: Kuni kohtumiseni?\\nRob: Muidugi :)\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f66c282-18b5-4cce-a298-169c7e5eb7b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric ja Rob l√§hevad vaatama stand-up'i YouTube'is.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][1]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e317fc-241a-4af6-88c7-5a91ea347967",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√§rtsis langes peaministripartei toetus √ºhe protsendi v√µrra. M√§rtsis t√µusis SDP toetus 1,9 protsenti ja erakonna toetus 21 21,7 protsenti. M√§rtsis langes kasvas toetust naiste ja Koonderakonna toetus √ºhe protsendi v√µrra. Koonderakonna toetus 1 protsendi v√µrra. M√§rtsis. M√§rtsis langes t√µusis langes SDP toetus √ºhe protsendi v√µrra. M√§rtsis langes M√§rtsis langes M√§rtsis langes SDP toetus √ºhe protsendi v√µrra. M√§rtsis langes SDP toetus 1, M√§rtsis t√µusis SDP toetus 1, M√§rtsis t√µusis SDP toetus 1, SDP toetus 1, SDP toetus 1, M√§rtsis 1, SDP toetus 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1 9 protsenti ja erakonna toetus 1, M√§rtsis 1, SDP toetus 1, SDP toetus 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis. M√§rtsis 1, SDP suurendas 1, SDP suurendas. M√§rtsis 1, SDP suurendas. M√§rtsis 1, SDP suurendas. M√§rtsis 1, SDP suurendas. M√§rtsis 1, SDP suurendas. M√§rtsis 1, SDP suurendas. M√§rtsis 1, SDP. M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, SDP. M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, SDP. M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, SDP. M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis 1, M√§rtsis\n"
     ]
    }
   ],
   "source": [
    "input_text=\"\"\"Veel veebruaris oli Soome k√µige populaarsem partei Koonderakond, kuid kaotas m√§rtsis selle tiitli SDP-le. M√§rtsis langes peaministripartei  toetus √ºhe protsendi v√µrra ning Koonderakonna toetus on n√º√ºd 20,6 protsenti.\n",
    "SDP suurendas toetust naiste ja noorte hulgas. M√§rtsis t√µusis SDP toetus 1,9 protsenti ning erakonna toetus on n√º√ºd 21,7 protsenti. \n",
    "Koonderakonna kannul on P√µlissoomlased, rahandusminister Riikka Purra kodupartei toetus on 17,4 protsenti.\"\"\"\n",
    "summarize(input_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50844dcd-1a70-4965-86d8-837473c187b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenny peaks valima esimese v√µi kolmanda paari. Bob saadab talle pilte. Lennyle meeldib k√µige rohkem esimesed p√ºksid, kuid talle meeldivad k√µige rohkem esimesed lillad p√ºksid.\n",
      "CPU times: user 481 ms, sys: 271 ¬µs, total: 482 ms\n",
      "Wall time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "%time summarize(dataset['test'][2]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896f03f8-b54e-4fc4-b3f0-3e64ee158a86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma ja Will s√∂√∂vad t√§na √µhtust. Emma valmistab toitu. Emma j√µuab koju varsti. Will ei taha, et ta teda j√§rele tulla.\n",
      "CPU times: user 267 ms, sys: 0 ns, total: 267 ms\n",
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%time summarize(dataset['test'][3]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1279d7-2e2f-450e-aaf1-1245fec0c55e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}