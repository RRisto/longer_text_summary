{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde883fd-1389-47af-a85d-c9eb670dcd9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:878: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /home/risto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback, MBartConfig\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizerFast,MBartTokenizer,Trainer, TrainingArguments\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from hftrim.TokenizerTrimmer import TokenizerTrimmer\n",
    "from hftrim.ModelTrimmers import MBartTrimmer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "# from transformers.convert_slow_tokenizer import convert_slow_tokenizer\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from random import randrange\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from lsg_converter import LSGConverter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ea95d5-17ce-4915-b20a-c2119e3c6892",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902452d9-f3c2-4085-8fa6-5c7eaf94ea14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ccec9-1a90-4e52-93df-277c06e0dedd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e69b14-4a03-463e-b71b-79c6b800edc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf42950-9405-4d2a-bb56-46991f67c626",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c325f1-27e3-46af-9918-72289c66fad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9758d323-6b9d-4bdf-9ae1-19aaebb450a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6685f2-29ac-4018-9486-b33f823f014a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef187b1-9e0e-44ea-bf56-2c1b114dc6cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_id=\"facebook/mbart-large-cc25\"\n",
    "src_lang = \"et_EE\" # Example source language code\n",
    "tgt_lang = \"et_EE\" # Example target language code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df83dd6-353c-4e01-874b-1fb76d377560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_slow = MBartTokenizer.from_pretrained(model_id)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fe507c-b7b8-48ca-b7c5-3a5fbf57ab38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Important: Set both source and target languages\n",
    "tokenizer.src_lang = src_lang\n",
    "tokenizer.tgt_lang = tgt_lang\n",
    "\n",
    "tokenizer_slow.src_lang = src_lang\n",
    "tokenizer_slow.tgt_lang = tgt_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e0b57c-46de-424e-81a8-e67640e0d45e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41913215486098887\n",
      "0.41913215486098887\n"
     ]
    }
   ],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "#share of params in embedding\n",
    "print(msize(model.model.shared) / msize(model))   \n",
    "print(msize(model.lm_head) / msize(model))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2ca13-1507-402d-bb77-289d594c6219",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0488a3e2-a550-4014-9f58-25686dd78a8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"TalTechNLP/samsum_ee\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed5008b-5135-4442-9de1-565471151878",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train']=dataset['train'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "dataset['test'] = dataset['test'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607554e0-274a-417b-a5d9-12f19d42e9af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 13199\n",
      "Test dataset size: 809\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "841231ad-5ce9-40ee-a268-acf4e5201989",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 13199\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5124a695-a6c7-4825-bce6-681575fcd281",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'en_dialogue', 'en_summary'],\n",
       "    num_rows: 809\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset['test'].filter(lambda example, idx: example['summary'] is not None and example['dialogue'] is not None, with_indices=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10c473-40b5-4f9a-825d-ecb94eeef887",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## trim models vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33d3e93b-68bc-4acf-b5c9-aa9978e2e734",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26398"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts4vocab=train_dataset['dialogue']+train_dataset['summary']\n",
    "len(texts4vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f78824c-4d9e-4121-bcfa-0f08e6d3beec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = MBartConfig.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13803b0c-fbe6-49c5-bd71-914a36224c0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1981 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 250000/250000 [00:00<00:00, 1167225.66it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# trim tokenizer\n",
    "tokenizer_slow_trim = TokenizerTrimmer(tokenizer_slow)\n",
    "tokenizer_slow_trim.make_vocab(texts4vocab)\n",
    "tokenizer_slow_trim.make_tokenizer()\n",
    "\n",
    "# trim model\n",
    "model_trim = MBartTrimmer(model, config, tokenizer_slow_trim.trimmed_tokenizer)\n",
    "model_trim.make_weights(tokenizer_slow_trim.trimmed_vocab_ids)\n",
    "model_trim.make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715da82-494a-4ee3-b30f-14da9183aab0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save trimmed model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11147006-b556-4bd6-8317-a7d41556213c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_slow_trim.trimmed_tokenizer.save_pretrained('mbart-large-cc25_et')\n",
    "model_trim.trimmed_model.save_pretrained('mbart-large-cc25_et')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9eca1-6156-4581-a51e-0271b102f266",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e858cc4a-d4cc-4506-90be-8e7b825c0383",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = MBartTokenizer.from_pretrained('mbart-large-cc25_et')\n",
    "tokenizer = MBartTokenizerFast.from_pretrained(\"mbart-large-cc25_et/\", from_slow=True)\n",
    "tokenizer.src_lang = src_lang\n",
    "tokenizer.tgt_lang = tgt_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c67d1da-805a-4a30-a6da-cbee306fb1c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained('mbart-large-cc25_et', max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b421bdd-e71f-4f62-b83a-a0bc1a7ac1c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model and make it accepting longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20490531-b219-4b41-92cb-6553ea6e0fe1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1067: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/risto/.local/lib/python3.9/site-packages/transformers/configuration_utils.py:508: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/home/risto/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2759: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of LSGMBartForConditionalGeneration were not initialized from the model checkpoint at mbart-large-cc25_et and are newly initialized: ['model.encoder.global_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/risto/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lsg_converter.mbart.modeling_lsg_mbart.LSGMBartForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "converter = LSGConverter(max_sequence_length=2048)\n",
    "\n",
    "model_id='mbart-large-cc25_et'\n",
    "model, tokenizer = converter.convert_from_pretrained(model_id, num_global_tokens=7)\n",
    "tokenizer.src_lang = src_lang\n",
    "tokenizer.tgt_lang = tgt_lang\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b298d1ff-992e-4a20-99c4-c3aae20fb8c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('mbart-large-cc25_lsg_2048_et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2def0ad-aa26-41aa-bdc3-2ffefedb6bab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/tokenizer_config.json',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/special_tokens_map.json',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/sentencepiece.bpe.model',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/added_tokens.json',\n",
       " '/opt/work/storage2/localdata/datasience_dev/text/summarization/transformers/mbart-large-cc25_lsg_2048_et/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('mbart-large-cc25_lsg_2048_et')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c1b3b-d201-49bf-9362-85cfc93c3366",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## prep data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58462ab6-a200-4870-9404-116e6e9dd483",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_source_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f000e83a-938f-4062-bbd9-81e49581f3bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_target_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3737edea-df3e-4261-9141-ad2c8368fbd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d4b93f9-c7fd-4683-b21d-15283e217b7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f766165-4f45-40a8-9898-c4a458584f01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc810e-4647-46f0-8813-a0ac8f5f5a9e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de471419-c631-41db-9055-485596a606d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bcf0960-da63-4f4f-a88b-6bd8cdce783d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c1268-ae7d-4537-9de0-f6b448a5af76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9394d21f-92d3-42fa-8597-a2ff3c8aad64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStoppingCallback(3, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c4ae189-8f5e-4add-8b34-4cf0c40c2ce6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risto/.local/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face repository id\n",
    "#save to other disc where chmod is supported\n",
    "repository_id = f\"mbart-large-cc25_lsg_2048_et_TalTechNLP/{dataset_id}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027c86c4-b33e-429c-8a8a-e9d975162bcd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MBartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16500' max='33000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16500/33000 5:04:37 < 5:04:39, 0.90 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.069600</td>\n",
       "      <td>1.933898</td>\n",
       "      <td>36.339800</td>\n",
       "      <td>15.320700</td>\n",
       "      <td>30.846800</td>\n",
       "      <td>33.451100</td>\n",
       "      <td>29.572311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.522100</td>\n",
       "      <td>1.894038</td>\n",
       "      <td>37.566600</td>\n",
       "      <td>15.305900</td>\n",
       "      <td>31.708200</td>\n",
       "      <td>34.706200</td>\n",
       "      <td>26.925834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.085500</td>\n",
       "      <td>1.983595</td>\n",
       "      <td>37.673800</td>\n",
       "      <td>16.121600</td>\n",
       "      <td>32.103900</td>\n",
       "      <td>34.945300</td>\n",
       "      <td>25.632880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>2.337484</td>\n",
       "      <td>34.909000</td>\n",
       "      <td>14.002000</td>\n",
       "      <td>28.889000</td>\n",
       "      <td>32.182400</td>\n",
       "      <td>72.166873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.499300</td>\n",
       "      <td>2.539751</td>\n",
       "      <td>37.482400</td>\n",
       "      <td>15.062900</td>\n",
       "      <td>31.021300</td>\n",
       "      <td>34.480600</td>\n",
       "      <td>31.426452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16500, training_loss=1.2926952958540483, metrics={'train_runtime': 18278.2764, 'train_samples_per_second': 7.221, 'train_steps_per_second': 1.805, 'total_flos': 2.8045274990444544e+17, 'train_loss': 1.2926952958540483, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1459ff5-8265-4fab-a1f6-5a321128e221",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calc metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1dfb433-b776-4c97-b4cf-47c92b1243c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_id=\"mbart-large-cc25_lsg_2048_et_TalTechNLP/TalTechNLP/samsum_ee/checkpoint-6600/\"\n",
    "tokenizer = MBartTokenizerFast.from_pretrained('transformers/mbart-large-cc25_lsg_2048_et',\n",
    "                                               from_slow=True)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8116a954-a542-4638-a974-fa6bf94089ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model=model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f9f6b0-1ff8-485a-b16c-06543479dc88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443647/343644409.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  outputs = torch.tensor(padded_predictions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 37.0547,\n",
       " 'rouge2': 14.8011,\n",
       " 'rougeL': 30.5113,\n",
       " 'rougeLsum': 34.1101,\n",
       " 'gen_len': 456.85784919653895}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_texts_labels_metrics(texts, summaries, model, tokenizer, max_input_length=1024, max_output_length=512, batch_size = 10):\n",
    "    true_labels=tokenizer(\n",
    "        summaries, return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids.cpu()\n",
    "    input_ids = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_input_length\n",
    "    ).input_ids  \n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(0, input_ids.size(0), batch_size):\n",
    "        batch_input_ids = input_ids[i:i + batch_size].to('cuda')\n",
    "        batch_outputs = model.generate(input_ids=batch_input_ids, max_length=max_output_length)\n",
    "        predictions.extend(batch_outputs.cpu().detach().numpy())\n",
    "    max_length = max(len(p) for p in predictions)\n",
    "    padded_predictions = [np.pad(p, (0, max_length - len(p)), mode='constant') for p in predictions]\n",
    "    outputs = torch.tensor(padded_predictions)\n",
    "    eval_preds = (outputs, true_labels.cpu())\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    return metrics\n",
    "\n",
    "eval_metrics=calc_texts_labels_metrics(dataset['test']['dialogue'], dataset['test']['summary'], model, tokenizer)\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c751f-8c86-4371-b961-994df10e6767",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f99c6f-f858-4f42-baa1-7e0196bd9c98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda ei leia Betty telefoninumbrit. Hannah saadab talle sõnumi Larrylt. Larry helistas talle viimati, kui nad koos pargis oliid. Hannah ei tunne teda hästi.\n"
     ]
    }
   ],
   "source": [
    "def summarize(text, model, tokenizer, max_input_length=2048, max_new_tokens=512):\n",
    "    input_ids = tokenizer(\n",
    "         text, return_tensors=\"pt\",\n",
    "        max_length=max_input_length\n",
    "    ).input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids=input_ids.to('cuda'), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    \n",
    "summarize(dataset['test'][0]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e48a4fc-feb7-4307-9bf0-5f7449493932",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hannah: Hei, kas sul on Betty number?\\nAmanda: Vaatan järele.\\nHannah: <file_gif>\\nAmanda: Vabandust, ei leia seda.\\nAmanda: Küsi Larrylt.\\nAmanda: Ta helistas talle viimati, kui me koos pargis olime.\\nHannah: Ma ei tunne teda hästi.\\nHannah: <file_gif>\\nAmanda: Ära ole häbelik, ta on väga tore.\\nHannah: Kui sa ütled nii..\\nHannah: Ma eelistaksin, et sa talle sõnumi saadaksid.\\nAmanda: Lihtsalt saada talle sõnum 🙂\\nHannah: Urgh.. Olgu siis\\nHannah: Nägemist\\nAmanda: Nägemist-nägemist'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dataset['test'][0]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aabbb9e4-5dcc-4c8f-9035-ccec584f1249",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rob ja Eric vaatavad Masin'i. Nad kohtuvad siis.\n"
     ]
    }
   ],
   "source": [
    "summarize(dataset['test'][1]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd4fd15c-30c8-4ade-9f55-20687b6f3ec4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric: MASIN!\\nRob: See on nii lahe!\\nEric: Ma tean! Ja näitab, kuidas ameeriklased näevad venelasi ;)\\nRob: Ja see on tõesti naljakas!\\nEric: Ma tean! Mulle meeldib eriti rongi osa!\\nRob: Hahaha! Keegi ei räägi masinaga nii!\\nEric: Kas see on tema ainus etendus?\\nRob: Ei tea. Ma vaatan järele.\\nEric: Kindlasti.\\nRob: Selgub, et ei! Mõned tema etendused on Youtube'is.\\nEric: Lahe! Ma vaatan neid kohe!\\nRob: Mina ka!\\nEric: MASIN!\\nRob: MASIN!\\nEric: Kuni kohtumiseni?\\nRob: Muidugi :)\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f66c282-18b5-4cce-a298-169c7e5eb7b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eric ja Rob lähevad vaatama stand-up'i YouTube'is.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][1]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e317fc-241a-4af6-88c7-5a91ea347967",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Märtsis langes peaministripartei toetus ühe protsendi võrra. Märtsis tõusis SDP toetus 1,9 protsenti ja erakonna toetus 21 21,7 protsenti. Märtsis langes kasvas toetust naiste ja Koonderakonna toetus ühe protsendi võrra. Koonderakonna toetus 1 protsendi võrra. Märtsis. Märtsis langes tõusis langes SDP toetus ühe protsendi võrra. Märtsis langes Märtsis langes Märtsis langes SDP toetus ühe protsendi võrra. Märtsis langes SDP toetus 1, Märtsis tõusis SDP toetus 1, Märtsis tõusis SDP toetus 1, SDP toetus 1, SDP toetus 1, Märtsis 1, SDP toetus 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1 9 protsenti ja erakonna toetus 1, Märtsis 1, SDP toetus 1, SDP toetus 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis. Märtsis 1, SDP suurendas 1, SDP suurendas. Märtsis 1, SDP suurendas. Märtsis 1, SDP suurendas. Märtsis 1, SDP suurendas. Märtsis 1, SDP suurendas. Märtsis 1, SDP suurendas. Märtsis 1, SDP. Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, SDP. Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, SDP. Märtsis 1, Märtsis 1, Märtsis 1, SDP. Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis 1, Märtsis\n"
     ]
    }
   ],
   "source": [
    "input_text=\"\"\"Veel veebruaris oli Soome kõige populaarsem partei Koonderakond, kuid kaotas märtsis selle tiitli SDP-le. Märtsis langes peaministripartei  toetus ühe protsendi võrra ning Koonderakonna toetus on nüüd 20,6 protsenti.\n",
    "SDP suurendas toetust naiste ja noorte hulgas. Märtsis tõusis SDP toetus 1,9 protsenti ning erakonna toetus on nüüd 21,7 protsenti. \n",
    "Koonderakonna kannul on Põlissoomlased, rahandusminister Riikka Purra kodupartei toetus on 17,4 protsenti.\"\"\"\n",
    "summarize(input_text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50844dcd-1a70-4965-86d8-837473c187b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenny peaks valima esimese või kolmanda paari. Bob saadab talle pilte. Lennyle meeldib kõige rohkem esimesed püksid, kuid talle meeldivad kõige rohkem esimesed lillad püksid.\n",
      "CPU times: user 481 ms, sys: 271 µs, total: 482 ms\n",
      "Wall time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "%time summarize(dataset['test'][2]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896f03f8-b54e-4fc4-b3f0-3e64ee158a86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma ja Will söövad täna õhtust. Emma valmistab toitu. Emma jõuab koju varsti. Will ei taha, et ta teda järele tulla.\n",
      "CPU times: user 267 ms, sys: 0 ns, total: 267 ms\n",
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%time summarize(dataset['test'][3]['dialogue'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1279d7-2e2f-450e-aaf1-1245fec0c55e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}